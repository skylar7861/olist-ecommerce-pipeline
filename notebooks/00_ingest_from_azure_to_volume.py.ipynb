{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9591b141-19f1-47e6-98a8-b6d39839354a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 00_ingest_from_azure_to_dbfs (ONE-CELL VERSION)\n",
    "# Purpose:\n",
    "#   - Databricks Serverless (AWS) cannot reliably write to /dbfs via Python open()/Path().\n",
    "#   - So we download Azure blobs to local /tmp (Python I/O allowed),\n",
    "#     then copy them into DBFS using dbutils.fs.cp (supported).\n",
    "#\n",
    "# What you must fill in:\n",
    "#   TENANT_ID, CLIENT_ID, CLIENT_SECRET, STORAGE_ACCOUNT_NAME (if different)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Install dependencies (Azure SDK) and restart Python so imports work\n",
    "# ----------------------------\n",
    "\n",
    "# Install Azure identity + Azure blob storage SDK into the current notebook environment\n",
    "%pip install -q azure-identity azure-storage-blob\n",
    "\n",
    "# Restart the Python process so newly installed packages are available immediately\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6ed206-7aab-4c57-ad5c-6c04581ac5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78884674-9d86-4982-a328-ef9e028f37c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import azure.identity\n",
    "import azure.storage.blob\n",
    "print(\"Azure SDK imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2bfaa50-63a7-4273-a8e3-11685530de05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Download Azure blobs directly into YOUR Databricks Volume\n",
    "# Volume base path: /Volumes/workspace/default/olist_stage\n",
    "# ============================================================\n",
    "\n",
    "# Import Path so we can create folders and write files inside the Volume\n",
    "from pathlib import Path\n",
    "\n",
    "# Import datetime just for logging timestamps (optional)\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Azure credential class to authenticate using Service Principal (client id/secret)\n",
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "# Import Azure Blob client to list and download blobs (files) from the container\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Azure credentials + storage config (YOU MUST FILL THESE IN)\n",
    "# ----------------------------\n",
    "\n",
    "# Tenant ID (Directory ID) from Microsoft Entra ID (Azure AD)\n",
    "TENANT_ID = dbutils.secrets.get(\"olist-secrets\", \"tenant_id\")\n",
    "print(f\"Tenant ID: {TENANT_ID}\")\n",
    "\n",
    "\n",
    "# Client ID (Application (client) ID / appId) for your Service Principal\n",
    "CLIENT_ID = dbutils.secrets.get(\"olist-secrets\", \"client_id\")\n",
    "\n",
    "# Client Secret (password) for your Service Principal (keep private, do NOT commit)\n",
    "CLIENT_SECRET = dbutils.secrets.get(\"olist-secrets\", \"client_secret\")\n",
    "\n",
    "# Storage account name created by Terraform (example: olistlake89321)\n",
    "STORAGE_ACCOUNT_NAME = dbutils.secrets.get(\"olist-secrets\", \"storage_account\")\n",
    "\n",
    "\n",
    "# Container name created by Terraform (example: datalake)\n",
    "CONTAINER_NAME = \"datalake\"\n",
    "\n",
    "# Prefix inside the container where your raw CSVs were uploaded\n",
    "RAW_PREFIX = \"raw/olist/\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Databricks Volume destination (YOUR correct base path)\n",
    "# ----------------------------\n",
    "# New (under olist catalog)\n",
    "\n",
    "\n",
    "# Your Volume base path (you confirmed this path exists)\n",
    "VOLUME_BASE = \"/Volumes/olist/stage/olist_stage\"      # New volume location\n",
    "\n",
    "# A subfolder inside the volume to keep raw downloads organized\n",
    "VOLUME_DIR = f\"{VOLUME_BASE}/olist_raw\"              # Raw landing folder inside that volume\n",
    "\n",
    "\n",
    "# Create the folder if it does not exist (Volumes support normal filesystem operations)\n",
    "Path(VOLUME_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build Azure endpoint URL + authenticate (OAuth)\n",
    "# ----------------------------\n",
    "\n",
    "# Azure Blob endpoint for your storage account (HTTPS endpoint works cross-cloud)\n",
    "ACCOUNT_URL = f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "\n",
    "# Create a credential object for Azure OAuth token retrieval using client credentials flow\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=TENANT_ID,        # Which tenant to authenticate against\n",
    "    client_id=CLIENT_ID,        # Which application is authenticating\n",
    "    client_secret=CLIENT_SECRET # Password for the application\n",
    ")\n",
    "\n",
    "# Create a top-level client for Azure Blob Storage operations\n",
    "blob_service_client = BlobServiceClient(\n",
    "    account_url=ACCOUNT_URL,    # Storage account endpoint\n",
    "    credential=credential       # OAuth credential (Service Principal)\n",
    ")\n",
    "\n",
    "# Create a container client to interact with the specific container (datalake)\n",
    "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) List all blobs (files) under the prefix raw/olist/\n",
    "# ----------------------------\n",
    "\n",
    "# Collect blob names that begin with RAW_PREFIX (Azure uses prefixes like folders)\n",
    "blob_names = [b.name for b in container_client.list_blobs(name_starts_with=RAW_PREFIX)]\n",
    "\n",
    "# Print how many blobs we found (helps confirm prefix is correct)\n",
    "print(f\"[{datetime.utcnow().isoformat()}Z] Found {len(blob_names)} blobs under '{RAW_PREFIX}'\")\n",
    "\n",
    "# Print a few blob names as a quick sanity check\n",
    "print(\"Sample blobs:\", blob_names[:10])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Download each blob to the Volume folder\n",
    "# ----------------------------\n",
    "\n",
    "# Loop through each blob path returned by Azure\n",
    "for blob_name in blob_names:\n",
    "    # Skip placeholder blobs if present (some flows create raw/olist/.keep)\n",
    "    if blob_name.endswith(\"/.keep\"):\n",
    "        continue\n",
    "\n",
    "    # Extract the filename from the blob path (everything after the last slash)\n",
    "    filename = blob_name.split(\"/\")[-1]\n",
    "\n",
    "    # Build the destination path in the Volume\n",
    "    local_path = str(Path(VOLUME_DIR) / filename)\n",
    "\n",
    "    # Create a client object for this specific blob so we can download it\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "    # Download the blob as bytes\n",
    "    data = blob_client.download_blob().readall()\n",
    "\n",
    "    # Write the bytes directly into the Volume path (supported in Serverless)\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "    # Print progress so you can see what was downloaded\n",
    "    print(f\"Downloaded {blob_name} -> {local_path}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Verify files exist in the Volume\n",
    "# ----------------------------\n",
    "\n",
    "# List CSV files in the Volume download folder\n",
    "downloaded_csvs = sorted([p.name for p in Path(VOLUME_DIR).glob(\"*.csv\")])\n",
    "\n",
    "# Print downloaded filenames for verification\n",
    "print(\"Downloaded CSVs:\", downloaded_csvs)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Spark read test from the Volume path (validates end-to-end)\n",
    "# ----------------------------\n",
    "\n",
    "# Pick a known CSV to validate Spark can read what we downloaded\n",
    "test_file = \"olist_orders_dataset.csv\"\n",
    "\n",
    "# Build the full path to the file inside the Volume\n",
    "test_path = f\"{VOLUME_DIR}/{test_file}\"\n",
    "\n",
    "# Read the CSV using Spark to confirm it is usable for the Bronze pipeline\n",
    "df_test = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)       # Use first row as header\n",
    "    .option(\"inferSchema\", True)  # Infer column types for quick testing\n",
    "    .csv(test_path)               # Read from the Volume path\n",
    ")\n",
    "\n",
    "# Display a few rows in the notebook UI\n",
    "display(df_test.limit(5))\n",
    "\n",
    "# Print row count to confirm Spark can scan the file\n",
    "print(\"Row count:\", df_test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36acdd93-fa50-4a69-b6ea-15e863edbad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confirm the scope exists by reading one secret (wonâ€™t print the secret value)\n",
    "print(\"tenant_id length:\", len(dbutils.secrets.get(\"olist-secrets\", \"tenant_id\")))\n",
    "print(\"client_id length:\", len(dbutils.secrets.get(\"olist-secrets\", \"client_id\")))\n",
    "print(\"client_secret length:\", len(dbutils.secrets.get(\"olist-secrets\", \"client_secret\")))\n",
    "print(\"storage_account:\", dbutils.secrets.get(\"olist-secrets\", \"storage_account\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e159f959-fdcc-4ce2-90fa-091fa89597e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tenant_id = dbutils.secrets.get(\"olist-secrets\", \"tenant_id\")\n",
    "\n",
    "print(\"tenant_id raw repr:\", repr(tenant_id))\n",
    "print(\"tenant_id length:\", len(tenant_id))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5929882390114386,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_ingest_from_azure_to_volume.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
