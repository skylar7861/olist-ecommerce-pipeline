{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11970606-ca2b-4b80-bc01-a07772c640b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 01_bronze_ingest.py (FULL SCRIPT + REQUESTED IMPROVEMENTS)\n",
    "# Unity Catalog + Volumes compatible + Incremental scaffolding\n",
    "#\n",
    "# Changes applied (per your request):\n",
    "#   A) Split catalog/schema names for clarity (olist.bronze and olist.ops)\n",
    "#   B) Update ingest-log comment to match renamed bronze tables (orders_raw etc.)\n",
    "#   C) Add .option(\"overwriteSchema\",\"true\") to Bronze writes for safer overwrite runs\n",
    "#\n",
    "# What this notebook does:\n",
    "#   - Reads raw CSVs from a Databricks Volume:\n",
    "#       /Volumes/workspace/default/olist_stage/olist_raw\n",
    "#   - Adds Bronze metadata columns:\n",
    "#       ingested_at, source_file, source_system\n",
    "#   - Writes to Unity Catalog managed Bronze tables using Delta + saveAsTable()\n",
    "#   - Maintains an ingest log table to support incremental runs:\n",
    "#       olist.ops.bronze_ingest_log\n",
    "#   - Incremental logic: when WRITE_MODE=\"append\", ingest only NEW source files\n",
    "#\n",
    "# How to run:\n",
    "#   - First run: WRITE_MODE = \"overwrite\"  (clean baseline)\n",
    "#   - Future incremental runs: WRITE_MODE = \"append\" (only new files)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Imports\n",
    "# ----------------------------\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp,  # Adds an ingestion timestamp column\n",
    "    lit,                # Adds constant values as columns\n",
    "    col                 # References columns (including _metadata.file_path)\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Source location (Raw CSVs) - in Databricks Volume\n",
    "# ----------------------------\n",
    "\n",
    "# Raw CSV folder (already downloaded from Azure into this Volume)\n",
    "RAW_VOLUME_DIR = \"/Volumes/olist/stage/olist_stage/olist_raw\"\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Catalog/Schema configuration (explicit and clear)\n",
    "# ----------------------------\n",
    "\n",
    "# Bronze layer catalog and schema (database)\n",
    "BRONZE_CATALOG = \"olist\"       # Catalog name you created\n",
    "BRONZE_DB = \"bronze\"           # Schema (database) name inside the catalog\n",
    "BRONZE_SCHEMA = f\"{BRONZE_CATALOG}.{BRONZE_DB}\"  # Fully-qualified schema, e.g. olist.bronze\n",
    "\n",
    "# Ops layer catalog and schema (database) for control tables\n",
    "OPS_CATALOG = \"olist\"          # Same catalog\n",
    "OPS_DB = \"ops\"                 # Ops schema name inside the catalog\n",
    "OPS_SCHEMA = f\"{OPS_CATALOG}.{OPS_DB}\"  # Fully-qualified schema, e.g. olist.ops\n",
    "\n",
    "# Control table to track which source files have already been ingested per bronze table\n",
    "INGEST_LOG_TABLE = f\"{OPS_SCHEMA}.bronze_ingest_log\"  # Fully-qualified table name\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Write mode configuration\n",
    "# ----------------------------\n",
    "\n",
    "# Write mode:\n",
    "#   - \"overwrite\" = initial full load (clean baseline)\n",
    "#   - \"append\"    = incremental loads (only new files)\n",
    "WRITE_MODE = \"append\"  # Change to \"append\" later for incremental runs\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Create schemas (databases) if they don't exist\n",
    "# ----------------------------\n",
    "\n",
    "# Create the Bronze schema under the 'olist' catalog\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Create the Ops schema under the 'olist' catalog\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {OPS_SCHEMA}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Create ingest log table (control table) if not exist\n",
    "# ----------------------------\n",
    "\n",
    "# This table stores one row per (bronze_table, source_file) that has been ingested.\n",
    "# It is used to prevent re-ingesting the same file in incremental runs.\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {INGEST_LOG_TABLE} (\n",
    "  bronze_table STRING,      -- e.g. 'orders_raw'\n",
    "  source_file  STRING,      -- UC-safe file path from _metadata.file_path\n",
    "  ingested_at  TIMESTAMP    -- timestamp when we ingested the file\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Helper: Read CSV and add Unity Catalogâ€“safe metadata columns\n",
    "# ----------------------------\n",
    "\n",
    "def read_csv_with_metadata(csv_filename: str):\n",
    "    \"\"\"\n",
    "    Read a CSV from the raw volume directory and add standard Bronze metadata columns.\n",
    "    Uses _metadata.file_path (Unity Catalog compatible) for file lineage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build full path to the CSV file in the raw volume folder\n",
    "    csv_path = f\"{RAW_VOLUME_DIR}/{csv_filename}\"\n",
    "\n",
    "    # Read the CSV file into a Spark DataFrame\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)       # Treat first row as header\n",
    "        .option(\"inferSchema\", True)  # Infer datatypes (ok for Bronze)\n",
    "        .csv(csv_path)                # Read CSV from the Volume path\n",
    "    )\n",
    "\n",
    "    # Add standard Bronze metadata columns\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"ingested_at\", current_timestamp())          # When this row was ingested\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))   # UC-safe source file path\n",
    "        .withColumn(\"source_system\", lit(\"olist_kaggle\"))        # Constant source label\n",
    "    )\n",
    "\n",
    "    # Return dataframe with metadata\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Helper: Filter only NEW files for incremental runs\n",
    "# ----------------------------\n",
    "\n",
    "def filter_new_files_only(df, bronze_table: str):\n",
    "    \"\"\"\n",
    "    If WRITE_MODE == 'append', filter out rows whose source_file has already been ingested\n",
    "    for this bronze_table (based on INGEST_LOG_TABLE).\n",
    "    If WRITE_MODE == 'overwrite', return df unchanged (full reload).\n",
    "    \"\"\"\n",
    "\n",
    "    # If full reload, do not filter anything\n",
    "    if WRITE_MODE == \"overwrite\":\n",
    "        return df\n",
    "\n",
    "    # Load previously ingested file paths for this bronze table from the ingest log\n",
    "    logged_files = (\n",
    "        spark.table(INGEST_LOG_TABLE)                              # Read ingest log table\n",
    "        .filter(col(\"bronze_table\") == lit(bronze_table))           # Only this bronze table\n",
    "        .select(col(\"source_file\").alias(\"logged_source_file\"))     # Rename for join clarity\n",
    "        .distinct()                                                 # Unique file paths only\n",
    "    )\n",
    "\n",
    "    # Left anti join keeps rows from df that do NOT match logged file paths\n",
    "    df_new = (\n",
    "        df.join(\n",
    "            logged_files,                                           # DataFrame of already-ingested files\n",
    "            on=(df[\"source_file\"] == logged_files[\"logged_source_file\"]),  # Match on file path\n",
    "            how=\"left_anti\"                                         # Keep only rows that do not match\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return only rows coming from new, unseen files\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Helper: Write Bronze table as Unity Catalog MANAGED table (no LOCATION)\n",
    "# ----------------------------\n",
    "\n",
    "def write_bronze_table_managed(df, bronze_table: str):\n",
    "    \"\"\"\n",
    "    Write a DataFrame as a Unity Catalog managed Delta table using saveAsTable().\n",
    "    This avoids CREATE TABLE ... LOCATION '/Volumes/...', which triggers:\n",
    "      'Missing cloud file system scheme'\n",
    "    \"\"\"\n",
    "\n",
    "    # Build fully-qualified UC table name (catalog.schema.table)\n",
    "    full_table_name = f\"{BRONZE_SCHEMA}.{bronze_table}\"\n",
    "\n",
    "    # Write as a managed Delta table:\n",
    "    # - overwrite: rebuild table fully (initial load)\n",
    "    # - append: add new rows (incremental)\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")                         # Delta Lake format\n",
    "        .mode(WRITE_MODE)                        # overwrite or append\n",
    "        .option(\"overwriteSchema\", \"true\")       # Allow schema updates on overwrite runs\n",
    "        .saveAsTable(full_table_name)            # UC managed table (Databricks manages storage)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Helper: Update ingest log after writing new rows\n",
    "# ----------------------------\n",
    "\n",
    "def log_ingested_files(df_written, bronze_table: str):\n",
    "    \"\"\"\n",
    "    Record which source_file values were ingested for this bronze_table.\n",
    "    This enables incremental runs to skip already processed files.\n",
    "    \"\"\"\n",
    "\n",
    "    # If overwrite, clear the existing log entries for this bronze table\n",
    "    # because we're rebuilding the table from scratch.\n",
    "    if WRITE_MODE == \"overwrite\":\n",
    "        spark.sql(f\"DELETE FROM {INGEST_LOG_TABLE} WHERE bronze_table = '{bronze_table}'\")\n",
    "\n",
    "    # Build a dataframe of distinct source files we ingested in this run\n",
    "    files_df = (\n",
    "        df_written\n",
    "        .select(\n",
    "            lit(bronze_table).alias(\"bronze_table\"),  # Tag with bronze table name\n",
    "            col(\"source_file\")                        # File path from metadata\n",
    "        )\n",
    "        .distinct()                                   # One row per file\n",
    "        .withColumn(\"ingested_at\", current_timestamp())  # When the file was ingested\n",
    "    )\n",
    "\n",
    "    # Append these file entries to the ingest log table\n",
    "    (\n",
    "        files_df.write\n",
    "        .format(\"delta\")                 # Store log as Delta\n",
    "        .mode(\"append\")                  # Always append new log entries\n",
    "        .saveAsTable(INGEST_LOG_TABLE)   # Write into the UC log table\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Dataset map (bronze_table -> csv filename)\n",
    "# ----------------------------\n",
    "\n",
    "datasets = {\n",
    "    # Bronze renamed tables (raw landing) - consistent naming\n",
    "    \"orders_raw\": \"olist_orders_dataset.csv\",\n",
    "    \"order_items_raw\": \"olist_order_items_dataset.csv\",\n",
    "    \"order_payments_raw\": \"olist_order_payments_dataset.csv\",\n",
    "    \"customers_raw\": \"olist_customers_dataset.csv\",\n",
    "    \"products_raw\": \"olist_products_dataset.csv\",\n",
    "    \"sellers_raw\": \"olist_sellers_dataset.csv\",\n",
    "    \"order_reviews_raw\": \"olist_order_reviews_dataset.csv\",\n",
    "    \"geolocation_raw\": \"olist_geolocation_dataset.csv\",\n",
    "    \"category_translation_raw\": \"product_category_name_translation.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Ingest loop (read -> incremental filter -> write -> log)\n",
    "# ----------------------------\n",
    "\n",
    "for bronze_table, csv_file in datasets.items():\n",
    "    # Read CSV and add Bronze metadata columns\n",
    "    df = read_csv_with_metadata(csv_file)\n",
    "\n",
    "    # Filter out already ingested files if running incrementally\n",
    "    df_new = filter_new_files_only(df, bronze_table)\n",
    "\n",
    "    # If there are no new rows, skip writing/logging to save compute\n",
    "    # NOTE: This triggers a small Spark job per table, but dataset is small and this is fine.\n",
    "    if df_new.limit(1).count() == 0:\n",
    "        print(f\"SKIP: {BRONZE_SCHEMA}.{bronze_table} (no new files)\")\n",
    "        continue\n",
    "\n",
    "    # Write the new rows to a UC managed Delta table\n",
    "    write_bronze_table_managed(df_new, bronze_table)\n",
    "\n",
    "    # Log ingested files so future append runs won't reprocess them\n",
    "    log_ingested_files(df_new, bronze_table)\n",
    "\n",
    "    # Print success message\n",
    "    print(f\"OK: {BRONZE_SCHEMA}.{bronze_table} written with mode={WRITE_MODE} from {csv_file}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 12) Validation: show row counts + recent ingest log entries\n",
    "# ----------------------------\n",
    "\n",
    "# Show row counts for each bronze table (quick sanity check)\n",
    "for bronze_table in datasets.keys():\n",
    "    display(\n",
    "        spark.sql(\n",
    "            f\"SELECT '{BRONZE_SCHEMA}.{bronze_table}' AS table_name, COUNT(*) AS row_count \"\n",
    "            f\"FROM {BRONZE_SCHEMA}.{bronze_table}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Show the most recent ingest log entries so you can confirm logging works\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"SELECT * FROM {INGEST_LOG_TABLE} ORDER BY ingested_at DESC LIMIT 50\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f7f7402-8d53-408c-b941-1c91b3b10d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "moving catalog from workspace to olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa63551c-e9fa-4014-91ce-863e66401b72",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768767360908}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "checking catalog"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Show which catalog your session is currently using (important for where schemas were created)\n",
    "SELECT current_catalog(); --> showing workspace\n",
    "\n",
    "-- Show which schema your session is currently using\n",
    "SELECT current_schema(); --> showing default\n",
    "\n",
    "-- -- List all catalogs you have access to (to confirm 'olist' doesn't already exist)\n",
    "-- SHOW CATALOGS;\n",
    "\n",
    "-- -- List schemas in the current catalog (to see where 'olist_bronze' and 'olist_ops' exist)\n",
    "-- SHOW SCHEMAS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af8e706-b1c7-4195-9ccb-e4c0b5d4a430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the new catalog\n",
    "CREATE CATALOG IF NOT EXISTS olist;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50040b3-9cca-481b-9d9a-b8647faabcc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create schemas inside the new catalog\n",
    "CREATE SCHEMA IF NOT EXISTS olist.bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS olist.ops;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abd2384-c5f9-4d69-a757-e0e000d9473a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;\n",
    "SHOW SCHEMAS IN olist;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759c0c5e-0f30-4b97-a04c-3898f1629667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE olist.bronze.orders_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_orders;\n",
    "\n",
    "CREATE TABLE olist.bronze.order_items_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_order_items;\n",
    "\n",
    "CREATE TABLE olist.bronze.order_payments_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_order_payments;\n",
    "\n",
    "CREATE TABLE olist.bronze.customers_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_customers;\n",
    "\n",
    "CREATE TABLE olist.bronze.products_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_products;\n",
    "\n",
    "CREATE TABLE olist.bronze.sellers_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_sellers;\n",
    "\n",
    "CREATE TABLE olist.bronze.order_reviews_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_order_reviews;\n",
    "\n",
    "CREATE TABLE olist.bronze.geolocation_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_geolocation;\n",
    "\n",
    "CREATE TABLE olist.bronze.category_translation_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_category_translation;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c8d20e-9817-4a17-b4c2-ef23c7607ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE olist.ops.bronze_ingest_log\n",
    "DEEP CLONE workspace.olist_ops.bronze_ingest_log;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0dee43-9921-4ba9-a440-007b8ace9697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN olist.bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c89787a-eb11-4c64-8665-d06717c9fbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/olist/stage/olist_stage/olist_raw/\"))\n",
    "dbutils.fs.ls(\"/Volumes/olist/stage/olist_stage/olist_raw\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6155563616779333,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingest.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
