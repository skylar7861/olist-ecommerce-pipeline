{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11970606-ca2b-4b80-bc01-a07772c640b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 01_bronze_ingest.py\n",
    "#\n",
    "# PURPOSE\n",
    "# -------\n",
    "# Ingest raw Olist CSV files into Unity Catalog \"Bronze\" tables.\n",
    "#\n",
    "# BRONZE DESIGN PRINCIPLES (WHY THIS IS BUILT THIS WAY)\n",
    "# -----------------------------------------------------\n",
    "# 1) Bronze is RAW and APPEND-SAFE:\n",
    "#    - We want to land data exactly as received, with minimal assumptions.\n",
    "#\n",
    "# 2) NO schema inference in Bronze:\n",
    "#    - Spark schema inference can guess wrong types (e.g., an INT column that\n",
    "#      later contains a timestamp string). That causes failures downstream.\n",
    "#    - Therefore, we store ALL columns as STRING in Bronze and cast types in Silver.\n",
    "#\n",
    "# 3) Unity Catalog managed tables:\n",
    "#    - We write using saveAsTable() so Databricks/UC manages storage locations.\n",
    "#    - This avoids LOCATION issues and works well with Serverless compute.\n",
    "#\n",
    "# 4) File-based incremental scaffolding:\n",
    "#    - We maintain an ingest log table that tracks which source files were ingested.\n",
    "#    - On incremental runs, we only ingest rows from files not seen before.\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Imports\n",
    "# ----------------------------\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp  # Spark expression: current timestamp for ingestion audit\n",
    "from pyspark.sql.functions import lit  # Spark expression: create constant-value columns\n",
    "from pyspark.sql.functions import col  # Spark expression: reference DataFrame columns (incl. _metadata.file_path)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Configuration (paths + schemas)\n",
    "# ----------------------------\n",
    "\n",
    "# Raw CSV landing directory inside a Unity Catalog Volume\n",
    "# This is where the Azureâ†’Volume ingestion notebook downloaded the CSVs.\n",
    "RAW_VOLUME_DIR = \"/Volumes/olist/stage/olist_stage/olist_raw\"\n",
    "\n",
    "# Unity Catalog schema for Bronze tables (raw landing tables)\n",
    "BRONZE_SCHEMA = \"olist.bronze\"\n",
    "\n",
    "# Unity Catalog schema for operational/control tables (ingest logs, checkpoints)\n",
    "OPS_SCHEMA = \"olist.ops\"\n",
    "\n",
    "# Ingest log table tracks which files have been ingested per Bronze table (for incremental runs)\n",
    "INGEST_LOG_TABLE = f\"{OPS_SCHEMA}.bronze_ingest_log\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Run mode (overwrite vs append)\n",
    "# ----------------------------\n",
    "\n",
    "# Overwrite is correct for the first full load / rebuild.\n",
    "# Append is used for future incremental runs (only new files are ingested).\n",
    "WRITE_MODE = \"overwrite\"  # Change to \"append\" after your first successful baseline load\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Ensure schemas exist (idempotent)\n",
    "# ----------------------------\n",
    "\n",
    "# Create the Bronze schema if it does not exist (safe to re-run)\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Create the Ops schema if it does not exist (safe to re-run)\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {OPS_SCHEMA}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Create ingest log table if missing (idempotent)\n",
    "# ----------------------------\n",
    "\n",
    "# This control table stores one row per (bronze_table, source_file) that was ingested.\n",
    "# It enables \"file-based idempotency\" so incremental runs won't reprocess the same file.\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {INGEST_LOG_TABLE} (\n",
    "  bronze_table STRING,      -- e.g. 'orders_raw'\n",
    "  source_file  STRING,      -- file path from UC metadata (_metadata.file_path)\n",
    "  ingested_at  TIMESTAMP    -- timestamp when that file was ingested\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Helper: Read a CSV as ALL STRINGS + add metadata\n",
    "# ----------------------------\n",
    "\n",
    "def read_csv_all_strings(csv_filename: str):\n",
    "    \"\"\"\n",
    "    Read a CSV from RAW_VOLUME_DIR with schema inference OFF.\n",
    "    This forces ALL columns to STRING so Bronze is stable and raw.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the full path to the CSV in the Volume\n",
    "    csv_path = f\"{RAW_VOLUME_DIR}/{csv_filename}\"\n",
    "\n",
    "    # Read the CSV file with header = True and inferSchema = False (everything becomes STRING)\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)          # Use the first row as column names\n",
    "        .option(\"inferSchema\", False)    # Critical: keep raw columns as STRING\n",
    "        .option(\"mode\", \"PERMISSIVE\")    # Do not fail on malformed rows; keep best-effort parsing\n",
    "        .csv(csv_path)                   # Load the file into a Spark DataFrame\n",
    "    )\n",
    "\n",
    "    # Add standard Bronze metadata columns:\n",
    "    # - ingested_at: when this row landed in Bronze\n",
    "    # - source_file: which input file produced this row (UC-safe metadata field)\n",
    "    # - source_system: constant label describing where data came from\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"ingested_at\", current_timestamp())         # Audit timestamp for ingestion\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))  # UC-safe file lineage (replaces input_file_name())\n",
    "        .withColumn(\"source_system\", lit(\"olist_kaggle\"))       # Constant source label\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame ready to be written to Bronze\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Helper: Filter only NEW files for incremental runs\n",
    "# ----------------------------\n",
    "\n",
    "def filter_new_files_only(df, bronze_table: str):\n",
    "    \"\"\"\n",
    "    If WRITE_MODE == 'append', drop rows whose source_file was already ingested (based on ingest log).\n",
    "    If WRITE_MODE == 'overwrite', return df unchanged (full rebuild).\n",
    "    \"\"\"\n",
    "\n",
    "    # Full rebuild: ingest everything\n",
    "    if WRITE_MODE == \"overwrite\":\n",
    "        return df\n",
    "\n",
    "    # For incremental runs:\n",
    "    # - Pull the set of already-ingested files for this bronze_table\n",
    "    logged_files = (\n",
    "        spark.table(INGEST_LOG_TABLE)                              # Read the ingest log control table\n",
    "        .filter(col(\"bronze_table\") == lit(bronze_table))           # Keep only logs for this bronze_table\n",
    "        .select(col(\"source_file\").alias(\"logged_source_file\"))     # Select file paths; rename for join clarity\n",
    "        .distinct()                                                 # Unique file paths only\n",
    "    )\n",
    "\n",
    "    # Left anti join = keep rows from df that DO NOT match logged_files\n",
    "    df_new = df.join(\n",
    "        logged_files,                                               # Already ingested file paths\n",
    "        on=(df[\"source_file\"] == logged_files[\"logged_source_file\"]),# Match on source_file path\n",
    "        how=\"left_anti\"                                             # Keep only new/unseen files\n",
    "    )\n",
    "\n",
    "    # Return only rows from files that have not been processed before\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Helper: Write Bronze table as Unity Catalog managed Delta table\n",
    "# ----------------------------\n",
    "\n",
    "def write_bronze_table_managed(df, bronze_table: str):\n",
    "    \"\"\"\n",
    "    Write a DataFrame into a UC managed Delta table using saveAsTable().\n",
    "    We do NOT specify a LOCATION because UC manages table storage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build fully-qualified table name: olist.bronze.<table>\n",
    "    full_table_name = f\"{BRONZE_SCHEMA}.{bronze_table}\"\n",
    "\n",
    "    # Write the DataFrame as Delta into a managed UC table\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"delta\")                      # Store in Delta format\n",
    "        .mode(WRITE_MODE)                     # overwrite for baseline; append for incremental\n",
    "        .option(\"overwriteSchema\", \"true\")    # Allow schema changes on overwrite runs\n",
    "        .saveAsTable(full_table_name)         # Managed UC table write\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Helper: Log ingested files into control table\n",
    "# ----------------------------\n",
    "\n",
    "def log_ingested_files(df_written, bronze_table: str):\n",
    "    \"\"\"\n",
    "    Log the source_file values that were ingested in this run for this bronze_table.\n",
    "    This is how we prevent re-processing the same files on incremental runs.\n",
    "    \"\"\"\n",
    "\n",
    "    # If we rebuilt the Bronze table with overwrite, we also reset the log for that table\n",
    "    if WRITE_MODE == \"overwrite\":\n",
    "        spark.sql(f\"DELETE FROM {INGEST_LOG_TABLE} WHERE bronze_table = '{bronze_table}'\")\n",
    "\n",
    "    # Build one log row per distinct source_file that contributed data in this run\n",
    "    files_df = (\n",
    "        df_written\n",
    "        .select(\n",
    "            lit(bronze_table).alias(\"bronze_table\"),  # Which bronze table we ingested into\n",
    "            col(\"source_file\")                        # Which file path produced rows\n",
    "        )\n",
    "        .distinct()                                   # One row per file\n",
    "        .withColumn(\"ingested_at\", current_timestamp())# Log time\n",
    "    )\n",
    "\n",
    "    # Append these log rows into the ingest log table\n",
    "    (\n",
    "        files_df.write\n",
    "        .format(\"delta\")                 # Store log rows in Delta\n",
    "        .mode(\"append\")                  # Append-only log table\n",
    "        .saveAsTable(INGEST_LOG_TABLE)   # Write to UC table\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Dataset map (Bronze table name -> CSV filename)\n",
    "# ----------------------------\n",
    "\n",
    "# Each entry defines:\n",
    "# - the target Bronze table name (ending with _raw)\n",
    "# - the source CSV file name in the Volume raw folder\n",
    "datasets = {\n",
    "    \"orders_raw\": \"olist_orders_dataset.csv\",\n",
    "    \"order_items_raw\": \"olist_order_items_dataset.csv\",\n",
    "    \"order_payments_raw\": \"olist_order_payments_dataset.csv\",\n",
    "    \"customers_raw\": \"olist_customers_dataset.csv\",\n",
    "    \"products_raw\": \"olist_products_dataset.csv\",\n",
    "    \"sellers_raw\": \"olist_sellers_dataset.csv\",\n",
    "    \"order_reviews_raw\": \"olist_order_reviews_dataset.csv\",\n",
    "    \"geolocation_raw\": \"olist_geolocation_dataset.csv\",\n",
    "    \"category_translation_raw\": \"product_category_name_translation.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Main ingestion loop (read -> filter -> write -> log)\n",
    "# ----------------------------\n",
    "\n",
    "for bronze_table, csv_file in datasets.items():\n",
    "    # Read the raw CSV as all strings and add metadata columns\n",
    "    df = read_csv_all_strings(csv_file)\n",
    "\n",
    "    # Apply incremental filter (only new files) if WRITE_MODE is append\n",
    "    df_new = filter_new_files_only(df, bronze_table)\n",
    "\n",
    "    # If there is no new data to ingest, skip writing and logging to save compute\n",
    "    if df_new.limit(1).count() == 0:\n",
    "        print(f\"SKIP: {BRONZE_SCHEMA}.{bronze_table} (no new files)\")\n",
    "        continue\n",
    "\n",
    "    # Write the Bronze table as a UC managed Delta table\n",
    "    write_bronze_table_managed(df_new, bronze_table)\n",
    "\n",
    "    # Log which files were ingested so future runs can skip duplicates\n",
    "    log_ingested_files(df_new, bronze_table)\n",
    "\n",
    "    # Print a success message for visibility\n",
    "    print(f\"OK: {BRONZE_SCHEMA}.{bronze_table} written with mode={WRITE_MODE} from {csv_file}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Validation (counts + ingest log preview)\n",
    "# ----------------------------\n",
    "\n",
    "# Show row counts for each Bronze table as a quick validation\n",
    "for bronze_table in datasets.keys():\n",
    "    display(\n",
    "        spark.sql(\n",
    "            f\"SELECT '{BRONZE_SCHEMA}.{bronze_table}' AS table_name, COUNT(*) AS row_count \"\n",
    "            f\"FROM {BRONZE_SCHEMA}.{bronze_table}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Show the most recent ingest-log rows so you can confirm incremental tracking works\n",
    "display(\n",
    "    spark.sql(\n",
    "        f\"SELECT * FROM {INGEST_LOG_TABLE} ORDER BY ingested_at DESC LIMIT 50\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f7f7402-8d53-408c-b941-1c91b3b10d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "moving catalog from workspace to olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa63551c-e9fa-4014-91ce-863e66401b72",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768767360908}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "checking catalog"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Show which catalog your session is currently using (important for where schemas were created)\n",
    "SELECT current_catalog(); --> showing workspace\n",
    "\n",
    "-- Show which schema your session is currently using\n",
    "SELECT current_schema(); --> showing default\n",
    "\n",
    "-- -- List all catalogs you have access to (to confirm 'olist' doesn't already exist)\n",
    "-- SHOW CATALOGS;\n",
    "\n",
    "-- -- List schemas in the current catalog (to see where 'olist_bronze' and 'olist_ops' exist)\n",
    "-- SHOW SCHEMAS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af8e706-b1c7-4195-9ccb-e4c0b5d4a430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the new catalog\n",
    "CREATE CATALOG IF NOT EXISTS olist;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50040b3-9cca-481b-9d9a-b8647faabcc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create schemas inside the new catalog\n",
    "CREATE SCHEMA IF NOT EXISTS olist.bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS olist.ops;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abd2384-c5f9-4d69-a757-e0e000d9473a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;\n",
    "SHOW SCHEMAS IN olist;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759c0c5e-0f30-4b97-a04c-3898f1629667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE olist.bronze.orders_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_orders;\n",
    "\n",
    "CREATE TABLE olist.bronze.order_items_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_order_items;\n",
    "\n",
    "CREATE TABLE olist.bronze.order_payments_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_order_payments;\n",
    "\n",
    "CREATE TABLE olist.bronze.customers_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_customers;\n",
    "\n",
    "CREATE TABLE olist.bronze.products_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_products;\n",
    "\n",
    "CREATE TABLE olist.bronze.sellers_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_sellers;\n",
    "\n",
    "CREATE TABLE olist.bronze.order_reviews_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_order_reviews;\n",
    "\n",
    "CREATE TABLE olist.bronze.geolocation_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_geolocation;\n",
    "\n",
    "CREATE TABLE olist.bronze.category_translation_raw\n",
    "DEEP CLONE workspace.olist_bronze.brz_category_translation;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c8d20e-9817-4a17-b4c2-ef23c7607ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE olist.ops.bronze_ingest_log\n",
    "DEEP CLONE workspace.olist_ops.bronze_ingest_log;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0dee43-9921-4ba9-a440-007b8ace9697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN olist.bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c89787a-eb11-4c64-8665-d06717c9fbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/olist/stage/olist_stage/olist_raw/\"))\n",
    "dbutils.fs.ls(\"/Volumes/olist/stage/olist_stage/olist_raw\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6155563616779333,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingest.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
